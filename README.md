
# nanoGPT

The purpose of this repository is for me to learn new technologies in Transformer Networks. The code definitely isn't production-ready and it's mostly for educational purposes. My goal for these implementation is correctness and readability rather than speed. 

Implemented things:

- Grouped-Query Attention (GQA)
- FlashAttention
- Rotary Position Embeddings (RoPE)
- Multi-head Latent Attention (MLA)
